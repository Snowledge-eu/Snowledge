import sys
import os
import json
from pathlib import Path

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from llm.analyse import load_yaml, analyse

# Exemple de message format√© depuis MongoDB
EXAMPLE_MESSAGES = [
    "[2025-05-11 14:43] screechyroll: üöÄ Tech Behind Masterclasser\n\nMasterclasser est un bot Discord d√©velopp√© en Node.js avec Discord.js v14, pens√© pour automatiser l'acc√®s √† des contenus √©ducatifs de haute qualit√©.\n\nüéØ Il repose sur une architecture modulaire avec :\n‚Ä¢ Un syst√®me de r√¥les dynamique bas√© sur les interactions utilisateurs\n‚Ä¢ Une base de donn√©es PostgreSQL pour stocker les historiques d'acc√®s, les progr√®s et les scores\n‚Ä¢ Des webhooks pour se connecter √† des plateformes comme Notion, YouTube ou Google Drive\n‚Ä¢ Une API REST pour exposer les stats de sessions et r√©cup√©rer les ressources associ√©es aux cours\n\nüîê S√©curit√© : OAuth2 + permissions granulaires\n‚öôÔ∏è Scalabilit√© : H√©berg√© via Docker, avec un reverse proxy Nginx + auto-restart en cas de crash\n\nMasterclasser, c'est l'intelligence d'un prof, la rigueur d'un automate, et l'accessibilit√© d'un bot."
]

# ============
# Function: select_from_list
# ------------
# DESCRIPTION: Prompt the user to select an item from a list.
# PARAMS:
#   - items: list of str, items to choose from
#   - prompt: str, prompt message
# RETURNS: str, selected item
# ============
def select_from_list(items, prompt):
    for idx, item in enumerate(items):
        print(f"[{idx+1}] {item}")
    while True:
        choice = input(f"{prompt} (1-{len(items)}): ").strip()
        if choice.isdigit() and 1 <= int(choice) <= len(items):
            return items[int(choice)-1]
        print("Invalid choice. Try again.")

# ============
# Function: main
# ------------
# DESCRIPTION: Entry point for interactive test script. Lists models/prompts, lets user select, et propose un exemple de message format√©.
# PARAMS: None
# RETURNS: None
# ============
def main():
    llm_path = Path(__file__).parent.parent / 'llm' / 'llm_models.yaml'
    prompt_path = Path(__file__).parent.parent / 'llm' / 'prompt_models.yaml'
    llm_config = load_yaml(str(llm_path))
    prompt_config = load_yaml(str(prompt_path))

    # List all models
    all_models = []
    for section in ['llm_models', 'lrm_models', 'vlm_models']:
        all_models.extend([m['name'] for m in llm_config.get(section, [])])
    print("\nAvailable models:")
    model_name = select_from_list(all_models, "Select a model")

    # List all prompts
    all_prompts = list(prompt_config.get('prompt_models', {}).keys())
    print("\nAvailable prompts:")
    prompt_name = select_from_list(all_prompts, "Select a prompt")

    # Choix du mode de test
    print("\n[1] Utiliser l'exemple de message MongoDB fourni")
    print("[2] Saisie manuelle de messages")
    mode = input("Choisir une option (1-2): ").strip()
    if mode == '1':
        messages = EXAMPLE_MESSAGES
        print("\nMessage utilis√©:")
        print(messages[0])
    else:
        print("\nEnter your messages (one per line, empty line to finish):")
        messages = []
        while True:
            msg = input()
            if msg.strip() == '':
                break
            messages.append(msg)
        if not messages:
            print("No messages entered. Exiting.")
            return

    # Ask for streaming
    stream = input("Enable streaming output? (y/N): ").strip().lower() == 'y'

    try:
        if stream:
            print("\nStreaming output:")
            for chunk in analyse(model_name, prompt_name, messages, stream=True):
                print(chunk, end='', flush=True)
            print()
        else:
            result = analyse(model_name, prompt_name, messages)
            print("\nResult:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()